{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importar Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pyodbc\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn import model_selection\n",
    "import seaborn as sn\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from scipy import stats\n",
    "from sklearn.decomposition import PCA\n",
    "from numpy import percentile\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuração SQL Server e importação dataset treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Configurar ligação ao SQL Server\n",
    "\n",
    "conn = pyodbc.connect('Driver={ODBC Driver 17 for SQL Server};'\n",
    "                      'Server=.;' \n",
    "                      'Database=Datawarehouse;'\n",
    "                      'UID=ole;'\n",
    "                      'PWD=12qwaszx*')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Importar Dataset para o PYNB\n",
    "\n",
    "df = pd.read_sql_query('SELECT * FROM  ML_DATASET_FINAL',conn)\n",
    "df = df.drop([\"RESERVATION_COD\"],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig = df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tratar Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df_orig\n",
    "#df = df.dropna()\n",
    "df = df.fillna(df.median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformação Logaritmica para corrigir enviesamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[\"ABT\"] = np.log1p(df[\"ABT\"])\n",
    "#df[\"LOR\"] = np.log1p(df[\"LOR\"])\n",
    "#df[\"AVG_OCP_GLOBAL_RATE\"] = np.log1p(df[\"AVG_OCP_GLOBAL_RATE\"])\n",
    "#df[\"AVG_OCP_ZONE_RATE\"] = np.log1p(df[\"AVG_OCP_ZONE_RATE\"])\n",
    "#df[\"AVG_OCP_STATION_RATE\"] = np.log1p(df[\"AVG_OCP_STATION_RATE\"])\n",
    "#df[\"N_RESERVAS_ANTERIORES\"] = np.log1p(df[\"N_RESERVAS_ANTERIORES\"])\n",
    "#df[\"N_CARROS_DISPONIVEIS\"] = np.log1p(df[\"N_CARROS_DISPONIVEIS\"])\n",
    "#df[\"AVG_OCP_RATE_RESERVA\"] = np.log1p(df[\"AVG_OCP_RATE_RESERVA\"])\n",
    "#df[\"PRICE_DAY\"] = np.log1p(df[\"PRICE_DAY\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gerar novas colunas de Data para as 3 Datas (Reserva, Out, In)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ RESERVATION DATE ##########################\n",
    "\n",
    "## Converter para datetime\n",
    "df['RESERVATION_DATE'] = pd.to_datetime(df['RESERVATION_DATE'], \n",
    " format = '%Y%m%d', \n",
    " errors = 'coerce')\n",
    "\n",
    "##Gerar as várias colunas\n",
    "df['R_Year'] = df['RESERVATION_DATE'].dt.year\n",
    "df['R_Month'] = df['RESERVATION_DATE'].dt.month\n",
    "#df['R_Week'] = df['RESERVATION_DATE'].dt.week\n",
    "#df['R_Day'] = df['RESERVATION_DATE'].dt.day\n",
    "df['R_DayofWeek'] = df['RESERVATION_DATE'].dt.dayofweek\n",
    "\n",
    "#Droppar a coluna original\n",
    "df = df.drop(\"RESERVATION_DATE\",1)\n",
    "\n",
    "############### OUT DATE ##########################\n",
    "\n",
    "# Converter para datetime\n",
    "df['OUT_DATE'] = pd.to_datetime(df['OUT_DATE'], \n",
    " format = '%Y%m%d', \n",
    " errors = 'coerce')\n",
    "\n",
    "#Gerar as várias colunas\n",
    "df['O_Year'] = df['OUT_DATE'].dt.year\n",
    "df['O_Month'] = df['OUT_DATE'].dt.month\n",
    "#df['O_Week'] = df['OUT_DATE'].dt.week\n",
    "#df['O_Day'] = df['OUT_DATE'].dt.day\n",
    "df['O_DayofWeek'] = df['OUT_DATE'].dt.dayofweek\n",
    "\n",
    "#Droppar a coluna original\n",
    "df = df.drop(\"OUT_DATE\",1)\n",
    "\n",
    "############### IN DATE ########################\n",
    "\n",
    "df['IN_DATE'] = pd.to_datetime(df['IN_DATE'], \n",
    " format = '%Y%m%d', \n",
    " errors = 'coerce')\n",
    "\n",
    "#Gerar as várias colunas\n",
    "df['I_Year'] = df['IN_DATE'].dt.year\n",
    "df['I_Month'] = df['IN_DATE'].dt.month\n",
    "#df['I_Week'] = df['IN_DATE'].dt.week\n",
    "#df['I_Day'] = df['IN_DATE'].dt.day\n",
    "df['I_DayofWeek'] = df['IN_DATE'].dt.dayofweek\n",
    "\n",
    "#Droppar a coluna original\n",
    "df = df.drop(\"IN_DATE\",1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-hot encoding (criação de dummy attributes) para as Features Categóricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, columns=['VEHICLE_GROUP_COD'], prefix = ['VG'])\n",
    "df = pd.get_dummies(df, columns=['VECHICLE_CLUSTER'], prefix = ['VC'])\n",
    "df = pd.get_dummies(df, columns=['VEHICLE_SEGMENT'], prefix = ['VS'])\n",
    "\n",
    "df = pd.get_dummies(df, columns=['IN_STATION'], prefix = ['IS'])\n",
    "df = pd.get_dummies(df, columns=['IN_ZONE'], prefix = ['IZ'])\n",
    "df = pd.get_dummies(df, columns=['IN_ACTIVITY'], prefix = ['IA'])\n",
    "\n",
    "df = pd.get_dummies(df, columns=['OUT_STATION'], prefix = ['OS'])\n",
    "df = pd.get_dummies(df, columns=['OUT_ZONE'], prefix = ['OZ'])\n",
    "df = pd.get_dummies(df, columns=['OUT_ACTIVITY'], prefix = ['OA'])\n",
    "\n",
    "\n",
    "df = pd.get_dummies(df, columns=['CLIENT_TYPE'], prefix = ['CT'])\n",
    "df = pd.get_dummies(df, columns=['CLIENT_SUBTYPE'], prefix = ['CST'])\n",
    "df = pd.get_dummies(df, columns=['VECHICLE_TYPE'], prefix = ['VT'])\n",
    "\n",
    "# One Hot Encoding dos atributos da data\n",
    "\n",
    "df = pd.get_dummies(df, columns=['R_Year'], prefix = ['RY'])\n",
    "df = pd.get_dummies(df, columns=['R_Month'], prefix = ['RM'])\n",
    "df = pd.get_dummies(df, columns=['R_DayofWeek'], prefix = ['RD'])\n",
    "\n",
    "df = pd.get_dummies(df, columns=['I_Year'], prefix = ['IY'])\n",
    "df = pd.get_dummies(df, columns=['I_Month'], prefix = ['IM'])\n",
    "df = pd.get_dummies(df, columns=['I_DayofWeek'], prefix = ['ID'])\n",
    "\n",
    "df = pd.get_dummies(df, columns=['O_Year'], prefix = ['OY'])\n",
    "df = pd.get_dummies(df, columns=['O_Month'], prefix = ['OM'])\n",
    "df = pd.get_dummies(df, columns=['O_DayofWeek'], prefix = ['OD'])\n",
    "\n",
    "# Replace de square brackets no nome das colunas\n",
    "df.columns=df.columns.str.replace('[','')\n",
    "df.columns=df.columns.str.replace(']','')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Variance Threshold - Remover features com baixa variância"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1275711 entries, 0 to 1275710\n",
      "Data columns (total 28 columns):\n",
      " #   Column                        Non-Null Count    Dtype  \n",
      "---  ------                        --------------    -----  \n",
      " 0   ABT                           1275711 non-null  int64  \n",
      " 1   LOR                           1275711 non-null  int64  \n",
      " 2   N_RESERVAS_ANTERIORES         1275711 non-null  float64\n",
      " 3   N_CARROS_DISPONIVEIS          1275711 non-null  float64\n",
      " 4   AVG_OCP_RATE_RESERVA          1275711 non-null  float64\n",
      " 5   VC_Compactos                  1275711 non-null  uint8  \n",
      " 6   VC_Mini/Económicos            1275711 non-null  uint8  \n",
      " 7   VS_Compactos                  1275711 non-null  uint8  \n",
      " 8   VS_M+E                        1275711 non-null  uint8  \n",
      " 9   IS_RAC - Aeroporto Lisboa     1275711 non-null  uint8  \n",
      " 10  IZ_Lisboa                     1275711 non-null  uint8  \n",
      " 11  IZ_Porto                      1275711 non-null  uint8  \n",
      " 12  OS_RAC - Aeroporto Lisboa     1275711 non-null  uint8  \n",
      " 13  OZ_Lisboa                     1275711 non-null  uint8  \n",
      " 14  OZ_Porto                      1275711 non-null  uint8  \n",
      " 15  CT_Leisure                    1275711 non-null  uint8  \n",
      " 16  CT_Retail                     1275711 non-null  uint8  \n",
      " 17  CST_Leisure                   1275711 non-null  uint8  \n",
      " 18  CST_Retail                    1275711 non-null  uint8  \n",
      " 19  RY_2017                       1275711 non-null  uint8  \n",
      " 20  RY_2018                       1275711 non-null  uint8  \n",
      " 21  RY_2019                       1275711 non-null  uint8  \n",
      " 22  IY_2017                       1275711 non-null  uint8  \n",
      " 23  IY_2018                       1275711 non-null  uint8  \n",
      " 24  IY_2019                       1275711 non-null  uint8  \n",
      " 25  OY_2017                       1275711 non-null  uint8  \n",
      " 26  OY_2018                       1275711 non-null  uint8  \n",
      " 27  OY_2019                       1275711 non-null  uint8  \n",
      "dtypes: float64(3), int64(2), uint8(23)\n",
      "memory usage: 76.6 MB\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
    "sel.fit(X)\n",
    "X_transformed_VT = X.loc[:, sel.get_support()]\n",
    "X_transformed_VT.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Univariate Feature Selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 5.31 GiB for an array with shape (559, 1275711) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-ee0cf892df17>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mselector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSelectKBest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmutual_info_regression\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mselector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# Get columns to keep and create new dataframe with those only\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 353\u001b[1;33m         \u001b[0mscore_func_ret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    354\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore_func_ret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscores_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpvalues_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscore_func_ret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m                           FutureWarning)\n\u001b[0;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_mutual_info.py\u001b[0m in \u001b[0;36mmutual_info_regression\u001b[1;34m(X, y, discrete_features, n_neighbors, copy, random_state)\u001b[0m\n\u001b[0;32m    366\u001b[0m            \u001b[0mof\u001b[0m \u001b[0ma\u001b[0m \u001b[0mRandom\u001b[0m \u001b[0mVector\u001b[0m\u001b[0;31m\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mProbl\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mPeredachi\u001b[0m \u001b[0mInf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m23\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1987\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m9\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m     \"\"\"\n\u001b[1;32m--> 368\u001b[1;33m     return _estimate_mi(X, y, discrete_features, False, n_neighbors,\n\u001b[0m\u001b[0;32m    369\u001b[0m                         copy, random_state)\n\u001b[0;32m    370\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_mutual_info.py\u001b[0m in \u001b[0;36m_estimate_mi\u001b[1;34m(X, y, discrete_features, discrete_target, n_neighbors, copy, random_state)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    274\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdiscrete_target\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 275\u001b[1;33m             X[:, continuous_mask] = scale(X[:, continuous_mask],\n\u001b[0m\u001b[0;32m    276\u001b[0m                                           with_mean=False, copy=False)\n\u001b[0;32m    277\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 5.31 GiB for an array with shape (559, 1275711) and data type float64"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "# Create and fit selector\n",
    "#selector = SelectKBest(f_regression, k=5)\n",
    "selector = SelectKBest(mutual_info_regression, k=30)\n",
    "\n",
    "selector.fit(X, y)\n",
    "\n",
    "# Get columns to keep and create new dataframe with those only\n",
    "cols = selector.get_support(indices=True)\n",
    "X_transformed_UFS = X.iloc[:,cols]\n",
    "X_transformed_UFS.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1275711 entries, 0 to 1275710\n",
      "Data columns (total 5 columns):\n",
      " #   Column              Non-Null Count    Dtype\n",
      "---  ------              --------------    -----\n",
      " 0   VC_Mini/Económicos  1275711 non-null  uint8\n",
      " 1   VC_Premium          1275711 non-null  uint8\n",
      " 2   VS_7&9L             1275711 non-null  uint8\n",
      " 3   VS_Automáticos      1275711 non-null  uint8\n",
      " 4   VS_M+E              1275711 non-null  uint8\n",
      "dtypes: uint8(5)\n",
      "memory usage: 6.1 MB\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Scaling e tratamento de Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Tratar Outliers e Scaling \n",
    "\n",
    "trans = RobustScaler()\n",
    "df[[\"ABT\",\"LOR\", \"AVG_OCP_GLOBAL_RATE\" ,\"AVG_OCP_ZONE_RATE\",\"AVG_OCP_STATION_RATE\", \"N_RESERVAS_ANTERIORES\",\"N_CARROS_DISPONIVEIS\",\"AVG_OCP_RATE_RESERVA\"]] = trans.fit_transform(df[[\"ABT\",\"LOR\", \"AVG_OCP_GLOBAL_RATE\", \"AVG_OCP_ZONE_RATE\", \"AVG_OCP_STATION_RATE\", \"N_RESERVAS_ANTERIORES\",\"N_CARROS_DISPONIVEIS\",\"AVG_OCP_RATE_RESERVA\"]])\n",
    "\n",
    "# calculate interquartile range\n",
    "q25 = percentile(df[\"PRICE_DAY\"], 25)\n",
    "q75 = percentile(df[\"PRICE_DAY\"], 75)\n",
    "iqr = q75 - q25\n",
    "print('Percentiles: 25th=%.3f, 75th=%.3f, IQR=%.3f' % (q25, q75, iqr))\n",
    "\n",
    "# calculate the outlier cutoff\n",
    "cut_off = iqr * 1.5\n",
    "lower = q25 - cut_off\n",
    "upper = q75 + cut_off\n",
    "print(upper)\n",
    "print(lower)\n",
    "\n",
    "# Apagar outliers\n",
    "df = df[ (df[\"PRICE_DAY\"] >= lower) &  (df[\"PRICE_DAY\"] <=  upper) ] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Separar Dataframe Teste e Treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Separar Dataset em Feature Attributes (X) e Target Attribute (y) \n",
    "\n",
    "X = df.drop([\"PRICE_DAY\"],axis = 1) # Todas as features\n",
    "y = df[\"PRICE_DAY\"].copy()\n",
    "\n",
    "# Normalizar para otimizar fit da rede neuronal\n",
    "#X = tf.keras.utils.normalize(X, axis=1)\n",
    "\n",
    "#### Criar Dataset treino e teste\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.20, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "# feature extraction\n",
    "test = SelectKBest(score_func=f_classif, k=4)\n",
    "fit = test.fit(X, Y)\n",
    "# summarize scores\n",
    "set_printoptions(precision=3)\n",
    "print(fit.scores_)\n",
    "features = fit.transform(X)\n",
    "# summarize selected features\n",
    "print(features[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reduzir dimensionalidade - PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pca = PCA(n_components= 0.50)\n",
    "#X = pca.fit_transform(X)\n",
    "#pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Artificial Neural Network Bull Shit Supreme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criar o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()  # a basic sequential feed-forward model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criar as várias layers da Rede Neuronal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos começar por criar a Input Layer que deve ser igual ao número de features do nosso X_train. Por default usa-se como função de ativação tf.nn.relu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Dense(559, activation=tf.nn.relu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depois vamos criar duas Hidden Layers que vão permitir que a rede compreenda melhor as relações não lineares complexas do problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Dense(100, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(75, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(50, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(25, activation=tf.nn.relu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por fim temos a nossa Output Layer que contem apenas um node por se tratar de uma regressão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    " # A função de custo vai ser MAE e usamos o Stochastic Gradient Descent como otimizador\n",
    "model.compile(loss='mean_absolute_error', optimizer='sgd', metrics=['mse']) \n",
    "\n",
    "model.fit(X_train, Y_train, epochs=10)  # train the model\n",
    "\n",
    "x = model.evaluate(X_test, Y_test)  # evaluate the out of sample data with model\n",
    "print(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
